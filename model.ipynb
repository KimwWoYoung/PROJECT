{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 회귀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A199uDEcXDd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Sex  Length  Diameter  Height  Whole weight  Shucked weight  \\\n",
            "0       0   0.455     0.365   0.095        0.5140          0.2245   \n",
            "1       0   0.350     0.265   0.090        0.2255          0.0995   \n",
            "2       1   0.530     0.420   0.135        0.6770          0.2565   \n",
            "3       0   0.440     0.365   0.125        0.5160          0.2155   \n",
            "4       2   0.330     0.255   0.080        0.2050          0.0895   \n",
            "...   ...     ...       ...     ...           ...             ...   \n",
            "3871    1   0.565     0.450   0.165        0.8870          0.3700   \n",
            "3872    0   0.590     0.440   0.135        0.9660          0.4390   \n",
            "3873    0   0.600     0.475   0.205        1.1760          0.5255   \n",
            "3874    1   0.625     0.485   0.150        1.0945          0.5310   \n",
            "3875    0   0.710     0.555   0.195        1.9485          0.9455   \n",
            "\n",
            "      Viscera weight  Shell weight  Rings  \n",
            "0             0.1010        0.1500     15  \n",
            "1             0.0485        0.0700      7  \n",
            "2             0.1415        0.2100      9  \n",
            "3             0.1140        0.1550     10  \n",
            "4             0.0395        0.0550      7  \n",
            "...              ...           ...    ...  \n",
            "3871          0.2390        0.2490     11  \n",
            "3872          0.2145        0.2605     10  \n",
            "3873          0.2875        0.3080      9  \n",
            "3874          0.2610        0.2960     10  \n",
            "3875          0.3765        0.4950     12  \n",
            "\n",
            "[3876 rows x 9 columns]\n",
            "XGBOOST\n",
            "Mean Accuracy: 0.8609\n",
            "MSE: 3.9112\n",
            "R2 Score: 0.5462\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import pickle\n",
        "\n",
        "def load_dataset_regression():\n",
        "    \"\"\"\n",
        "    회귀 모델을 위한 데이터셋 전처리\n",
        "    \"\"\"\n",
        "    # 데이터셋 불러오기\n",
        "    data = pd.read_csv(r'./data/Regression_data.csv')\n",
        "\n",
        "    # 'Sex' 열을 원-핫 인코딩으로 변환\n",
        "    #data = pd.get_dummies(data, columns=['Sex'], drop_first=True)\n",
        "    data['Sex'] = data['Sex'].replace({'M':0, 'F':1, 'I':2})\n",
        "\n",
        "    # 이상치 제거를 위해 확인할 열 선택\n",
        "    columns_to_check = ['Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight', 'Rings']\n",
        "\n",
        "    # z-점수를 이용하여 이상치 제거\n",
        "    z_scores = stats.zscore(data[columns_to_check])\n",
        "    abs_z_scores = np.abs(z_scores)\n",
        "    filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
        "    data = data[filtered_entries]\n",
        "    df =data\n",
        "\n",
        "    # \"전체 무게 >= 조개껍질 벗긴 무게 + 내장 무게 + 껍질 무게\"를 만족하지 않는 행들 제거\n",
        "    df = df[df['Whole weight'] >= df['Shucked weight'] + df['Viscera weight'] + df['Shell weight']]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print(df)\n",
        "\n",
        "    # 전처리된 데이터를 반환\n",
        "    return df\n",
        "\n",
        "\n",
        "def xgboost(df):\n",
        "    y_target = df['Rings']\n",
        "    X_features = df.drop('Rings',axis=1, inplace=False)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156)\n",
        "\n",
        "    xgb_reg = XGBRegressor(n_estimators=400,\n",
        "                        learning_rate=0.01,\n",
        "                        colsample_bytree=0.8,\n",
        "                        subsample=0.2,\n",
        "                        gamma=0.2,\n",
        "                        max_depth=5,\n",
        "                        min_child_weight=5)\n",
        "\n",
        "    xgb_reg.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = xgb_reg.predict(X_test)\n",
        "\n",
        "    # 정확도 계산\n",
        "    acc = np.mean(1 - np.abs((y_pred - y_test) / y_test))\n",
        "    print('XGBOOST')\n",
        "    print(f\"Mean Accuracy: {acc:.4f}\")\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"MSE: {mse:.4f}\")\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print(f\"R2 Score: {r2:.4f}\")\n",
        "    return xgb_reg\n",
        "\n",
        "def main():\n",
        "    reg_dataset = load_dataset_regression()\n",
        "\n",
        "    a = xgboost(reg_dataset)\n",
        "    with open('regression.pkl', 'wb') as f:\n",
        "        pickle.dump(a, f)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 이진분류"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GN96TtROtQtM"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'RandomUnderSampler' from 'imblearn.over_sampling' (c:\\Users\\khan\\anaconda3\\envs\\sec6_pj1\\lib\\site-packages\\imblearn\\over_sampling\\__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m StandardScaler, LabelEncoder\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimblearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mover_sampling\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomUnderSampler\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mxgboost\u001b[39;00m \u001b[39mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n",
            "\u001b[1;31mImportError\u001b[0m: cannot import name 'RandomUnderSampler' from 'imblearn.over_sampling' (c:\\Users\\khan\\anaconda3\\envs\\sec6_pj1\\lib\\site-packages\\imblearn\\over_sampling\\__init__.py)"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from imblearn.over_sampling import RandomUnderSampler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pickle\n",
        "\n",
        "\n",
        "def load_dataset_binary_classification():\n",
        "    \"\"\"\n",
        "    이진 분류: 데이터 불균형 문제 (1 희소)\n",
        "    \"\"\"\n",
        "    global df\n",
        "    df = pd.read_csv(r'./data/binary_classification_data.csv')\n",
        "\n",
        "    df_1 = df.iloc[:, :-1]\n",
        "    standard_scaler = StandardScaler()\n",
        "    np_scaled = standard_scaler.fit_transform(df_1)\n",
        "    df_norm = pd.DataFrame(np_scaled, columns=list(df_1.columns))\n",
        "\n",
        "    # 이상치 제거\n",
        "    low, high = .05, .95\n",
        "    quantiles = df_norm.quantile([low, high])\n",
        "    quantile_norm = df_norm.apply(lambda col: col[(col >= quantiles.loc[low, col.name]) &\n",
        "                                                 (col <= quantiles.loc[high, col.name])], axis=0)\n",
        "    X = df_norm\n",
        "    targets = df['target_class']\n",
        "    le = LabelEncoder()\n",
        "    Y = le.fit_transform(targets)\n",
        "    Y = pd.Series(Y, name='target_class')\n",
        "\n",
        "    ros = RandomUnderSampler(random_state=0)\n",
        "    X_resampled, y_resampled = ros.fit_resample(X, Y)\n",
        "    df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "    df_resampled['targets'] = y_resampled\n",
        "    df = df_resampled\n",
        "\n",
        "    return df\n",
        "\n",
        "def binary_model(df):\n",
        "    # X, y 분리\n",
        "    X = df.drop(columns='targets')\n",
        "    y = df['targets']\n",
        "\n",
        "    # Train, Test Dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "    # Modeling - XGBoost Classifier\n",
        "    model = XGBClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Test\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred = np.round(y_pred).astype(int)\n",
        "\n",
        "    # Classification Report\n",
        "    model_pred = model.predict(X_test)\n",
        "    report = classification_report(y_test, model_pred)\n",
        "    accuracy = round(model.score(X_test, y_test) * 100, 1)\n",
        "\n",
        "    print(\"binary Report:\")\n",
        "    print(report)\n",
        "    print(f'BinaryClassifier: class 조절 정확도 (accuracy) {accuracy}%')\n",
        "\n",
        "    return binary_model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict_new_data(df, new_data, scaler):\n",
        "    # X, y 분리\n",
        "    X = df.drop(columns='targets')\n",
        "    y = df['targets']\n",
        "\n",
        "    # Modeling - XGBoost Classifier\n",
        "    model = XGBClassifier()\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Scaling the new_data using the provided scaler\n",
        "    new_data_scaled = scaler.transform(new_data)\n",
        "\n",
        "    # Probability estimates for the new data points\n",
        "    prediction_proba = model.predict_proba(new_data_scaled)\n",
        "\n",
        "    for idx, (pred, proba) in enumerate(zip(model.predict(new_data_scaled), prediction_proba)):\n",
        "        if pred == 1:\n",
        "            print(f\"Data point {idx+1}: Predicted Class: 1 (Pulsar), Probability: {proba[1]*100:.2f}%\")\n",
        "            a = round(proba[1]*100, 2)\n",
        "        else:\n",
        "            print(f\"Data point {idx+1}: Predicted Class: 0 (NOt Pulsar), Probability: {proba[0]*100:.2f}%\")\n",
        "            b = round(proba[0]*100, 2)\n",
        "    return a, b\n",
        "\n",
        "# NO Pulsar 표현 말고 다른 표현 좋은거 있으면 추천해주세요\n",
        "def main():\n",
        "    # 데이터셋 로드 및 전처리\n",
        "    df = load_dataset_binary_classification()\n",
        "\n",
        "    # 모델 학습 및 평가\n",
        "    a = binary_model(df)\n",
        "    with open('model_binary_class', 'wb') as f:\n",
        "        pickle.dump(a, f)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 새로운 데이터로 예측\n",
        "    main()\n",
        "    new_data = pd.DataFrame({\n",
        "        ' Mean of the integrated profile': [120, 60],\n",
        "        ' Standard deviation of the integrated profile': [50, 40],\n",
        "        ' Excess kurtosis of the integrated profile': [0, 3.0],\n",
        "        ' Skewness of the integrated profile': [0, 10],\n",
        "        ' Mean of the DM-SNR curve': [0, 10],\n",
        "        ' Standard deviation of the DM-SNR curve': [20, 70],\n",
        "        ' Excess kurtosis of the DM-SNR curve': [2, 10],\n",
        "        ' Skewness of the DM-SNR curve': [10, 110]\n",
        "    })\n",
        "\n",
        "    # Use the same StandardScaler used for training data\n",
        "    standard_scaler = StandardScaler()\n",
        "    X_scaled = standard_scaler.fit_transform(df.drop(columns='targets'))\n",
        "    a, b = predict_new_data(df, new_data, standard_scaler)\n",
        "    print(a, b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "binary Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99      3256\n",
            "           1       0.91      0.82      0.86       324\n",
            "\n",
            "    accuracy                           0.98      3580\n",
            "   macro avg       0.95      0.91      0.93      3580\n",
            "weighted avg       0.98      0.98      0.98      3580\n",
            "\n",
            "BinaryClassifier: class 조절 정확도 (accuracy) 97.7%\n",
            "Data point 1: Predicted Class: 1 (Pulsar), Probability: 66.03%\n",
            "Data point 2: Predicted Class: 1 (Pulsar), Probability: 98.28%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pickle\n",
        "\n",
        "\n",
        "def load_dataset_binary_classification():\n",
        "    \"\"\"\n",
        "    이진 분류: 데이터 불균형 문제 (1 희소)\n",
        "    \"\"\"\n",
        "    global df\n",
        "    df = pd.read_csv(r'./data/binary_classification_data.csv')\n",
        "\n",
        "    df_1 = df.iloc[:, :-1]\n",
        "    standard_scaler = StandardScaler()\n",
        "    np_scaled = standard_scaler.fit_transform(df_1)\n",
        "    df_norm = pd.DataFrame(np_scaled, columns=list(df_1.columns))\n",
        "\n",
        "    # 이상치 제거\n",
        "    low, high = .05, .95\n",
        "    quantiles = df_norm.quantile([low, high])\n",
        "    quantile_norm = df_norm.apply(lambda col: col[(col >= quantiles.loc[low, col.name]) &\n",
        "                                                 (col <= quantiles.loc[high, col.name])], axis=0)\n",
        "    X = df_norm\n",
        "    targets = df['target_class']\n",
        "    le = LabelEncoder()\n",
        "    Y = le.fit_transform(targets)\n",
        "    Y = pd.Series(Y, name='target_class')\n",
        "\n",
        "    df = pd.DataFrame(X, columns=X.columns)\n",
        "    df['targets'] = Y\n",
        "    return df\n",
        "\n",
        "def binary_model(df):\n",
        "    df_origin = df.copy()\n",
        "\n",
        "    X = df.drop(columns='targets')\n",
        "    Y = df['targets']\n",
        "\n",
        "    # RandomUnderSampler\n",
        "    ros = RandomUnderSampler(random_state=0)\n",
        "    X_resampled, y_resampled = ros.fit_resample(X, Y)\n",
        "    df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "    df_resampled['targets'] = y_resampled\n",
        "    df = df_resampled\n",
        "\n",
        "    # X, y 분리\n",
        "    X_origin = df_origin.drop(columns='targets')\n",
        "    y_origin = df_origin['targets']\n",
        "\n",
        "    # Train, Test Dataset\n",
        "    X_origin_train, X_origin_test, y_origin_train, y_origin_test = train_test_split(X_origin, y_origin, test_size=0.2, random_state=2)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n",
        "\n",
        "    # Modeling - XGBoost Classifier\n",
        "    model = XGBClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Classification Report\n",
        "    model_pred = model.predict(X_origin_test)\n",
        "    report = classification_report(y_origin_test, model_pred)\n",
        "    accuracy = round(model.score(X_origin_test, y_origin_test) * 100, 1)\n",
        "\n",
        "    print(\"binary Report:\")\n",
        "    print(report)\n",
        "    print(f'BinaryClassifier: class 조절 정확도 (accuracy) {accuracy}%')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def predict_new_data(df, new_data, scaler):\n",
        "    # X, y 분리\n",
        "    X = df.drop(columns='targets')\n",
        "    y = df['targets']\n",
        "\n",
        "    # Modeling - XGBoost Classifier\n",
        "    model = XGBClassifier()\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Scaling the new_data using the provided scaler\n",
        "    new_data_scaled = scaler.transform(new_data)\n",
        "\n",
        "    # Probability estimates for the new data points\n",
        "    prediction_proba = model.predict_proba(new_data_scaled)\n",
        "\n",
        "    for idx, (pred, proba) in enumerate(zip(model.predict(new_data_scaled), prediction_proba)):\n",
        "        if pred == 1:\n",
        "            print(f\"Data point {idx+1}: Predicted Class: 1 (Pulsar), Probability: {proba[1]*100:.2f}%\")\n",
        "        else:\n",
        "            print(f\"Data point {idx+1}: Predicted Class: 0 (NOt Pulsar), Probability: {proba[0]*100:.2f}%\")\n",
        "\n",
        "# NO Pulsar 표현 말고 다른 표현 좋은거 있으면 추천해주세요\n",
        "\n",
        "def main():\n",
        "    # 데이터셋 로드 및 전처리\n",
        "    df = load_dataset_binary_classification()\n",
        "    \n",
        "    #모델 학습 및 평가\n",
        "    a = binary_model(df)\n",
        "    with open('binary.pkl', 'wb') as f:\n",
        "        pickle.dump(a, f)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 새로운 데이터로 예측\n",
        "    main()\n",
        "    new_data = pd.DataFrame({\n",
        "        ' Mean of the integrated profile': [120, 60],\n",
        "        ' Standard deviation of the integrated profile': [50, 40],\n",
        "        ' Excess kurtosis of the integrated profile': [0, 3.0],\n",
        "        ' Skewness of the integrated profile': [0, 10],\n",
        "        ' Mean of the DM-SNR curve': [0, 10],\n",
        "        ' Standard deviation of the DM-SNR curve': [20, 70],\n",
        "        ' Excess kurtosis of the DM-SNR curve': [2, 10],\n",
        "        ' Skewness of the DM-SNR curve': [10, 110]\n",
        "    })\n",
        "    # Use the same StandardScaler used for training data\n",
        "    standard_scaler = StandardScaler()\n",
        "    X_scaled = standard_scaler.fit_transform(df.drop(columns='targets'))\n",
        "    \n",
        "    # 스케일러 객체 저장\n",
        "    with open('binary_class_scaler.pkl', 'wb') as f:\n",
        "        pickle.dump(standard_scaler, f)\n",
        "    \n",
        "    predict_new_data(df, new_data, standard_scaler)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 다중 분류"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fwJNthg308VX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Features: Index(['X_Minimum', 'Pixels_Areas', 'Length_of_Conveyer',\n",
            "       'Steel_Plate_Thickness', 'Outside_X_Index', 'LogOfAreas', 'Log_X_Index',\n",
            "       'Orientation_Index'],\n",
            "      dtype='object')\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.81      0.80       124\n",
            "           1       0.99      0.97      0.98       144\n",
            "           2       0.98      0.95      0.97       126\n",
            "           3       0.82      0.78      0.80       134\n",
            "           4       0.88      0.91      0.89       139\n",
            "           5       0.99      1.00      1.00       134\n",
            "           6       0.97      0.99      0.98       142\n",
            "\n",
            "    accuracy                           0.92       943\n",
            "   macro avg       0.92      0.92      0.92       943\n",
            "weighted avg       0.92      0.92      0.92       943\n",
            "\n",
            "RandomForestClassifier: class 조절 정확도 (accuracy) 91.73%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import pickle\n",
        "\n",
        "\n",
        "def load_dataset_multi_classification(): \n",
        "    df = pd.read_csv(r'./data/mulit_classification_data.csv')\n",
        "\n",
        "    # 컬럼 전처리\n",
        "    df['type'] = 'TypeOfSteel_A300'\n",
        "    df.loc[df['TypeOfSteel_A400'] == 1, 'type'] = 'TypeOfSteel_A400'\n",
        "    df.drop(['TypeOfSteel_A300', 'TypeOfSteel_A400'], axis=1, inplace=True)\n",
        "    df['type'].replace({\"TypeOfSteel_A300\":0,\"TypeOfSteel_A400\":1},inplace=True)\n",
        "\n",
        "    # X_Perimeter + Y_Perimeter = Total_Perimeter \n",
        "    df['Total_Perimeter'] = df['X_Perimeter'] + df['Y_Perimeter']\n",
        "    df.drop(['X_Perimeter', 'Y_Perimeter'], axis=1, inplace=True)\n",
        "\n",
        "    # Mean_of_Luminosity 컬럼으로 합치기\n",
        "    df['Mean_of_Luminosity'] = (df['Minimum_of_Luminosity'] + df['Maximum_of_Luminosity']) / 2\n",
        "    df.drop(['Minimum_of_Luminosity', 'Maximum_of_Luminosity'], axis=1, inplace=True)\n",
        "\n",
        "    # target 데이터 -> int bool 타입으로 변경 \n",
        "    target_df = [\n",
        "        df['Pastry'],\n",
        "        df['Z_Scratch'],\n",
        "        df['K_Scatch'],\n",
        "        df['Stains'],\n",
        "        df['Dirtiness'],\n",
        "        df['Bumps'],\n",
        "        df['Other_Faults'] \n",
        "    ]\n",
        "    targets = list(map(lambda i: i.astype(bool), target_df))\n",
        "    choices = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains', 'Dirtiness', 'Bumps', 'Other_Faults']\n",
        "    df.drop(df[choices].columns, axis=1, inplace=True)\n",
        "    df['class'] = np.select(targets, choices)\n",
        "\n",
        "    # class외의 독립변수 데이터 표준화\n",
        "    df_1 = df.iloc[:, :-1]\n",
        "    # StandardScaler 객체 생성\n",
        "    standard_scaler = StandardScaler()\n",
        "    np_scaled = standard_scaler.fit_transform(df_1)\n",
        "    df_norm = pd.DataFrame(np_scaled, columns=list(df_1.columns))\n",
        "\n",
        "    # 이상치 제거 \n",
        "    low, high = .05, .95\n",
        "    quantiles = df_norm.quantile([low, high])\n",
        "    quantile_norm = df_norm.apply(lambda col: col[(col >= quantiles.loc[low, col.name]) & \n",
        "                                        (col <= quantiles.loc[high, col.name])], axis=0)\n",
        "\n",
        "    # 상관계수 행렬 생성\n",
        "    corr_matrix = df_norm.corr().abs()\n",
        "    # 상삼각 행렬 부분(대각선 기준으로 위쪽)만 남기기 위해 적용\n",
        "    under = corr_matrix * (np.triu(np.ones(corr_matrix.shape), k=1))\n",
        "    # 상관계수가 0.95보다 큰 변수들 찾아서 제거\n",
        "    to_drop = [column for column in under.columns if any(under[column] > 0.95)]\n",
        "    df_norm = df_norm.drop(df_norm[to_drop], axis=1)\n",
        "\n",
        "    # target 데이터 LabelEncoder\n",
        "    X = df_norm\n",
        "    le = LabelEncoder()\n",
        "\n",
        "    # df_norm DataFrame에서 'class' 컬럼을 범주형 타겟 데이터로 사용\n",
        "    targets = df['class']\n",
        "    Y = le.fit_transform(targets)\n",
        "\n",
        "    # X와 Y를 하나의 데이터프레임으로 합치기 위해 Y를 Series로 변환하고, 열 이름을 'target'으로 지정\n",
        "    Y = pd.Series(Y, name='targets')\n",
        "\n",
        "    # 클래스 비중 조절을 위한 RandomOverSampler 객체 생성\n",
        "    smote = SMOTE(random_state=0)\n",
        "\n",
        "    # 클래스 비중 조절을 위해 fit_resample() 메서드를 사용하여 X_train, y_train을 샘플링\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, Y)\n",
        "\n",
        "    # X_resampled와 y_resampled를 DataFrame으로 변환\n",
        "    df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "\n",
        "    # 'targets' 컬럼 추가\n",
        "    df_resampled['targets'] = y_resampled\n",
        "    df = df_resampled\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def multi_classification(df):  \n",
        "    # target 데이터 LabelEncoder\n",
        "    X = df.drop(\"targets\", axis=1)\n",
        "    y = df['targets']\n",
        "\n",
        "    # 데이터를 훈련용(train)과 테스트용(test)으로 분리\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # RandomForestClassifier 모델 생성\n",
        "    model = RandomForestClassifier(n_estimators=300, \n",
        "                                   max_depth=30,\n",
        "                                   max_features='sqrt',\n",
        "                                   min_samples_leaf=1,\n",
        "                                   min_samples_split=5,\n",
        "                                   bootstrap=False,\n",
        "                                   class_weight='balanced',\n",
        "                                   random_state=42)\n",
        "\n",
        "    # 모델 학습\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Feature Importance 확인\n",
        "    feature_importance = model.feature_importances_\n",
        "    feature_names = X_train.columns\n",
        "\n",
        "    # 중요도가 낮은 피처 제거\n",
        "    threshold = 0.05  # 임계값 (임의로 설정, 조정 가능)\n",
        "    selected_features = feature_names[feature_importance <= threshold]\n",
        "    \n",
        "    # 임계값보다 작은 Feature Importance를 가진 피처들만 출력\n",
        "    #print(\"Features with Importance <= Threshold:\")\n",
        "    #for feature in selected_features:\n",
        "    #print(feature)\n",
        "\n",
        "    X_train_selected = X_train.drop(selected_features, axis=1)\n",
        "    X_test_selected = X_test.drop(selected_features, axis=1)\n",
        "\n",
        "    # 모델 다시 학습\n",
        "    model.fit(X_train_selected, y_train)\n",
        "\n",
        "    # 모델 평가 및 결과 출력\n",
        "    model_pred = model.predict(X_test_selected)\n",
        "    report = classification_report(y_test, model_pred)\n",
        "    accuracy = round(model.score(X_test_selected, y_test) * 100, 2)\n",
        "\n",
        "    print(\"Selected Features:\", X_train_selected.columns)\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "    print(f'RandomForestClassifier: class 조절 정확도 (accuracy) {accuracy}%')\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    df = load_dataset_multi_classification()\n",
        "    a = multi_classification(df)\n",
        "    with open('model_classification', 'wb') as f:\n",
        "        pickle.dump(a, f)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
